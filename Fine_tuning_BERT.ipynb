{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOq0GRZDsxXG8XvvWAJM3NN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/SSL/blob/main/Fine_tuning_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP : Fine-tuning de BERT pour la classification de textes\n",
        "\n",
        "Ce TP a pour objectif d'illustrer l'utilisation de BERT pour une t√¢che de classification de textes en utilisant le transfer learning. Nous utiliserons le dataset AG News qui contient des articles de presse √† classifier en 4 cat√©gories.\n"
      ],
      "metadata": {
        "id": "pP9LiSZmDdJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XCaoPUA--DY",
        "outputId": "fe6ec71e-e879-4701-d377-4bd1c4503c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pr√©paration de l'environnement\n",
        "\n",
        "Commen√ßons par importer les biblioth√®ques n√©cessaires et v√©rifier notre environnement d'ex√©cution.\n"
      ],
      "metadata": {
        "id": "-lHT3YS6Drap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cellule 1 : Imports et setup\n",
        "import torch\n",
        "from torch import nn\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# V√©rification du GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device utilis√© : {device}\")"
      ],
      "metadata": {
        "id": "8SzDXJJqCA6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Exploration et visualisation des donn√©es\n",
        "\n",
        "Explorons maintenant le dataset AG News pour comprendre sa structure et sa distribution.\n"
      ],
      "metadata": {
        "id": "e-0uWnAbDvey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du dataset AG News\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "print(\"Structure du dataset :\")\n",
        "print(dataset)\n",
        "\n",
        "# Affichage de quelques exemples\n",
        "print(\"\\nExemples du dataset :\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nExemple {i+1}:\")\n",
        "    print(f\"Texte : {dataset['train'][i]['text']}\")\n",
        "    print(f\"Label : {dataset['train'][i]['label']}\")\n",
        "\n",
        "# Statistiques sur les labels\n",
        "labels = [x['label'] for x in dataset['train']]\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(\"\\nDistribution des classes :\")\n",
        "for label, count in zip(unique, counts):\n",
        "    print(f\"Classe {label}: {count} exemples\")\n",
        "\n",
        "# Visualisation de la distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x=labels)\n",
        "plt.title(\"Distribution des classes dans le jeu d'entra√Ænement\")\n",
        "plt.xlabel(\"Classe\")\n",
        "plt.ylabel(\"Nombre d'exemples\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1vR2CnJ2CB5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tokenization avec BERT\n",
        "\n",
        "La tokenization est une √©tape cruciale dans le traitement du texte avec BERT. Nous allons explorer comment BERT d√©coupe le texte en tokens.\n"
      ],
      "metadata": {
        "id": "mlrglxz8DzJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Exemple de tokenization\n",
        "text = \"Hello! How are you doing? üòä\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f\"Texte original : {text}\")\n",
        "print(f\"Tokens : {tokens}\")\n",
        "\n",
        "# Conversion en IDs\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "print(f\"\\nInput IDs : {input_ids}\")\n",
        "print(f\"Tokens d√©cod√©s : {tokenizer.convert_ids_to_tokens(input_ids)}\")\n",
        "\n",
        "# Exemple complet de preprocessing\n",
        "encoding = tokenizer.encode_plus(\n",
        "    text,\n",
        "    add_special_tokens=True,\n",
        "    max_length=32,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "print(\"\\nSortie complete du tokenizer :\")\n",
        "print(f\"Input IDs shape : {encoding['input_ids'].shape}\")\n",
        "print(f\"Attention mask shape : {encoding['attention_mask'].shape}\")\n",
        "print(f\"Attention mask : {encoding['attention_mask'][0]}\")"
      ],
      "metadata": {
        "id": "7e4SHXjkCP7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pr√©paration des donn√©es\n",
        "\n",
        "Cr√©ation d'un Dataset personnalis√© pour g√©rer nos donn√©es efficacement."
      ],
      "metadata": {
        "id": "EBtuwzJoD4ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AGNewsDataset(Dataset):\n",
        "    def __init__(self, split, tokenizer, max_len=128):\n",
        "        self.dataset = load_dataset(\"ag_news\")[split]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.dataset[idx]['text'])\n",
        "        label = self.dataset[idx]['label']\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "60QVUNlbCaYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Architecture du mod√®le\n",
        "\n",
        "D√©finition de notre mod√®le de classification bas√© sur BERT.\n",
        "Points cl√©s de l'architecture :\n",
        "- Utilisation de BERT pr√©-entra√Æn√© comme base\n",
        "- Ajout d'une couche de classification\n",
        "- Gel des param√®tres de BERT pour le transfer learning"
      ],
      "metadata": {
        "id": "0UfA6AdoD91o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=4, freeze_bert=True):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        ############# Code ##############\n",
        "\n",
        "\n",
        "        #################################\n",
        "\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        ############# Code ##############\n",
        "\n",
        "\n",
        "        #################################\n",
        "\n",
        "\n",
        "\n",
        "# Cr√©ation du mod√®le et analyse des param√®tres\n",
        "model = BERTClassifier(num_classes=4, freeze_bert=True)\n",
        "\n",
        "# Analyse des param√®tres\n",
        "        ############# Code ##############\n",
        "\n",
        "\n",
        "        #################################\n",
        "\n",
        "print(\"Analyse des param√®tres du mod√®le :\")\n",
        "print(f\"Param√®tres totaux : {total_params:,}\")\n",
        "print(f\"Param√®tres entra√Ænables : {trainable_params:,}\")\n",
        "print(f\"Param√®tres gel√©s : {frozen_params:,}\")"
      ],
      "metadata": {
        "id": "tJh5MoqrCbaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Fonctions d'entra√Ænement et d'√©valuation\n",
        "\n",
        "Impl√©mentation des fonctions n√©cessaires pour l'entra√Ænement et l'√©valuation du mod√®le.\n",
        "Ces fonctions orchestrent :\n",
        "- L'entra√Ænement par batch\n",
        "- Le calcul de la loss et de l'accuracy\n",
        "- L'√©valuation sur le jeu de test\n",
        "- L'affichage de la progression"
      ],
      "metadata": {
        "id": "jFtowQ75EIZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc='Training')\n",
        "    for batch in progress_bar:\n",
        "        ############# Code ##############\n",
        "\n",
        "\n",
        "        #################################\n",
        "\n",
        "    return total_loss / len(data_loader), correct_predictions / total_predictions\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predictions = torch.max(outputs, dim=1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.shape[0]\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return (total_loss / len(data_loader),\n",
        "            correct_predictions / total_predictions,\n",
        "            all_predictions,\n",
        "            all_labels)"
      ],
      "metadata": {
        "id": "5rEIGEWJCpHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Entra√Ænement du mod√®le"
      ],
      "metadata": {
        "id": "fDDkK2LIER1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# Cr√©ation des dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataset = AGNewsDataset('test', tokenizer, max_len=128)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Pr√©paration du mod√®le\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Suivi des m√©triques\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'test_loss': [], 'test_acc': []\n",
        "}\n",
        "\n",
        "# Entra√Ænement\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "    ############# Code ##############\n",
        "\n",
        "\n",
        "    #################################"
      ],
      "metadata": {
        "id": "cnODP1XRCqVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Analyse des r√©sultats\n",
        "\n",
        "Visualisation et analyse des performances du mod√®le.\n",
        "Cette section nous permet de :\n",
        "- Visualiser les courbes d'apprentissage\n",
        "- Analyser la matrice de confusion\n",
        "- Identifier les forces et faiblesses du mod√®le"
      ],
      "metadata": {
        "id": "KtlDlMmvEWeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Courbes d'apprentissage\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train')\n",
        "plt.plot(history['test_loss'], label='Test')\n",
        "plt.title('Loss au cours des √©poques')\n",
        "plt.xlabel('√âpoque')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train')\n",
        "plt.plot(history['test_acc'], label='Test')\n",
        "plt.title('Accuracy au cours des √©poques')\n",
        "plt.xlabel('√âpoque')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Matrice de confusion\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(labels, predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matrice de confusion')\n",
        "plt.xlabel('Pr√©dictions')\n",
        "plt.ylabel('Vraies classes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-Rt1rdiGC5ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Test sur des exemples r√©els\n",
        "\n",
        "Utilisation du mod√®le entra√Æn√© sur de nouveaux exemples.\n",
        "Testez le mod√®le avec vos propres textes pour √©valuer ses performances dans des conditions r√©elles.\n"
      ],
      "metadata": {
        "id": "LnjaZnb6Ec7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "test_texts = [\n",
        "    \"NASA successfully launches new Mars rover to explore the red planet\",\n",
        "    \"Manchester United wins dramatic match against Liverpool\",\n",
        "    \"Apple stock reaches all-time high after strong quarterly earnings\",\n",
        "    \"New study reveals breakthrough in quantum computing research\"\n",
        "]\n",
        "\n",
        "############# Code ##############\n",
        "\n",
        "\n",
        "#################################"
      ],
      "metadata": {
        "id": "nJEXqjwtC9zs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}