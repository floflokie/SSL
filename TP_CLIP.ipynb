{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/floflokie/SSL/blob/main/TP_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfz7BSY7SP5P"
      },
      "source": [
        "# TP : apprentissage multimodal\n",
        "\n",
        "\n",
        "Dans ce TP, nous allons utiliser le modèle d'apprentissage, FashionCLIP, pré-entraîné sur des images ainsi que des descriptions en langage naturel. Plus particulièrement, nous allons considérer deux cas d'usage :\n",
        "\n",
        "*   **Moteur de recherche d'images :** il s'agit de trouver, à partir d'une requête en langage naturel, l'image correspondante.\n",
        "\n",
        "*   **Classification zero-shot :** il s'agit simplement de construire un classifieur d'images (faire correspondre un label à une image).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ3s403V5LKs"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Nous allons dans un premier temps télécharger les données. Celles-ci provienennt de [Kaggle](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLyWzNhJwoS2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gdown\n",
        "!gdown \"1igAuIEW_4h_51BG1o05WS0Q0-Cp17_-t&confirm=t\"\n",
        "!unzip data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dzpM2oASwM6"
      },
      "source": [
        "### Modèle FashionCLIP\n",
        "\n",
        "Nous allons également télécharger le modèle pré-entraîné."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyBLvPLgSx5h"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U fashion-clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WijCpqbIyH7T"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "#sys.path.append(\"fashion-clip/\")\n",
        "from fashion_clip.fashion_clip import FashionCLIP\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xEzYFUbydJY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "fclip = FashionCLIP('fashion-clip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViPu0y8C0UfS"
      },
      "source": [
        "FashionCLIP, à l'instar de CLIP, crée un espace vectoriel partagé pour les images et le texte. Cela permet de nombreuses applications, telles que la recherche (trouver l'image la plus similaire à une requête donnée) ou la classification zero-shot.\n",
        "\n",
        "Il y a principalement deux composants : un encodeur d'image (pour générer un vecteur à partir d'une image) et un encodeur de texte (pour générer un vecteur à partir d'un texte).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oc2jvxPpFWQ"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*FLNMtW6jK51fm7Og\"  width=\"400\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsPVQgNphwFX"
      },
      "source": [
        "Nous allons télécharger les données que nous allons ensuite nettoyer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9emW_P2fhxSW"
      },
      "outputs": [],
      "source": [
        "articles = pd.read_csv(\"data_for_fashion_clip/articles.csv\")\n",
        "\n",
        "# Supprimer les éléments ayant la même description\n",
        "subset = articles.drop_duplicates(\"detail_desc\").copy()\n",
        "\n",
        "# upprimer les images dont la catégrie n'est pas renseignée\n",
        "subset = subset[~subset[\"product_group_name\"].isin([\"Unknown\"])]\n",
        "\n",
        "# Garder seulement les descriptions dont la longueur est inférieure à 40 tokens\n",
        "subset = subset[subset[\"detail_desc\"].apply(lambda x : 4 < len(str(x).split()) < 40)]\n",
        "\n",
        "# Supprimer les articles qui ne sont pas suffisamment fréquents dans le jeu de données\n",
        "most_frequent_product_types = [k for k, v in dict(Counter(subset[\"product_type_name\"].tolist())).items() if v > 10]\n",
        "subset = subset[subset[\"product_type_name\"].isin(most_frequent_product_types)]\n",
        "\n",
        "subset.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTiCnV7Nko5L"
      },
      "outputs": [],
      "source": [
        "subset.to_csv(\"subset_data.csv\", index=False)\n",
        "f\"Il y a {len(subset)} éléments dans le dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcLNKhgD75pm"
      },
      "source": [
        "## Moteur de recherche d'images\n",
        "\n",
        "Constuire un moteur de recherche qui permet, à partir d'une description en langage naturel, de récupérer l'image correspondante. Mesurer ses performances (précision).\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cnKHgLAumVyuHuK9pkqr7A.gif\"  width=\"800\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cla9wews4eZg"
      },
      "outputs": [],
      "source": [
        "images = [\"data_for_fashion_clip/\" + str(k) + \".jpg\" for k in subset[\"article_id\"].tolist()]\n",
        "texts = subset[\"detail_desc\"].tolist()\n",
        "\n",
        "# Créer les représentations vectorielles (embeddings) des images et des descriptions.\n",
        "batch_size = 32\n",
        "image_embeddings = fclip.encode_images(images, batch_size=batch_size)\n",
        "text_embeddings = fclip.encode_text(texts, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5TKhC_NeKp3",
        "outputId": "fb93853f-e982-41b3-f221-effdb65fe1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3104, 512)\n",
            "(3104, 512)\n"
          ]
        }
      ],
      "source": [
        "print(image_embeddings.shape)\n",
        "print(text_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_engine(query):\n",
        "  query_embedding = fclip.encode_text([query])[0]\n",
        "  similarity = np.dot(image_embeddings, query_embedding) / (np.linalg.norm(image_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
        "  top_indices = np.argsort(similarity)[::-1][:5]\n",
        "  return [subset.iloc[i][\"article_id\"] for i in top_indices]\n",
        "\n",
        "image_search = search_engine(\"a pink dress\")\n",
        "print(image_search)\n",
        "for i in image_search:\n",
        "  display(Image.open(\"data_for_fashion_clip/\" + str(i) + \".jpg\"))"
      ],
      "metadata": {
        "id": "2-UN7pDcYk8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre moteur de recherche semble bien fonctionner."
      ],
      "metadata": {
        "id": "ES7bV687i4qo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87URDy7xh65d"
      },
      "source": [
        "# Classification zero-shot\n",
        "\n",
        "Construite un classsifieur d'images (prédire le label d'une image). Mesurer ses performances.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*No6ZONpQMIcfFaNMOI5oNw.gif\"  width=\"800\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilisera les types de produit comme labels."
      ],
      "metadata": {
        "id": "SDfvZo_2jlAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfgZVPv3jkbt",
        "outputId": "8793e767-7a8e-4ecf-caaf-be9c8023871e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Belt',\n",
              " 'Gloves',\n",
              " 'Other accessories',\n",
              " 'T-shirt',\n",
              " 'Socks',\n",
              " 'Garment Set',\n",
              " 'Underwear Tights',\n",
              " 'Bra',\n",
              " 'Necklace',\n",
              " 'Skirt',\n",
              " 'Blouse',\n",
              " 'Trousers',\n",
              " 'Dungarees',\n",
              " 'Top',\n",
              " 'Cap/peaked',\n",
              " 'Blazer',\n",
              " 'Boots',\n",
              " 'Hair/alice band',\n",
              " 'Shorts',\n",
              " 'Coat',\n",
              " 'Cardigan',\n",
              " 'Scarf',\n",
              " 'Sneakers',\n",
              " 'Ballerinas',\n",
              " 'Hoodie',\n",
              " 'Bag',\n",
              " 'Swimsuit',\n",
              " 'Hat/brim',\n",
              " 'Bikini top',\n",
              " 'Vest top',\n",
              " 'Dress',\n",
              " 'Earring',\n",
              " 'Polo shirt',\n",
              " 'Sandals',\n",
              " 'Jumpsuit/Playsuit',\n",
              " 'Leggings/Tights',\n",
              " 'Jacket',\n",
              " 'Shirt',\n",
              " 'Hat/beanie',\n",
              " 'Underwear bottom',\n",
              " 'Swimwear bottom',\n",
              " 'Other shoe',\n",
              " 'Bodysuit',\n",
              " 'Pyjama set',\n",
              " 'Sweater',\n",
              " 'Sunglasses']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On obtient de très bon résultats. On a un score de 0 sur top mais c'est un mot peu précis."
      ],
      "metadata": {
        "id": "LTsc5Chjk58E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci-dessous une démonstration du classifier."
      ],
      "metadata": {
        "id": "sVn4wL2vETP-"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}