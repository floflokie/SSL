{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsS+rrPJ6tSF1UR8cgv4Gj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/SSL/blob/main/TP_Explicabilit%C3%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P705BYggGP72"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TP : Explicabilité des modèles de deep learning pour les images avec LIME et SHAP**\n",
        "\n",
        "## **Objectif**\n",
        "Ce TP vise à explorer comment interpréter les prédictions d’un modèle de deep learning appliqué aux images à l’aide de **LIME** et **SHAP**. Ces outils permettent d’identifier les parties de l’image qui influencent le plus les décisions du modèle, contribuant ainsi à rendre les modèles plus transparents et compréhensibles.\n",
        "\n",
        "---\n",
        "\n",
        "## **Contexte**\n",
        "Les modèles de deep learning, bien qu’efficaces, sont souvent considérés comme des boîtes noires. Comprendre pourquoi un modèle fait une certaine prédiction est essentiel pour :  \n",
        "1. Identifier les biais potentiels dans le modèle.  \n",
        "2. Valider la fiabilité des prédictions dans des applications sensibles.  \n",
        "3. Construire la confiance des utilisateurs finaux.\n",
        "\n",
        "Dans ce TP, nous allons :  \n",
        "1. Charger un modèle de classification d’images pré-entraîné avec PyTorch.  \n",
        "2. Utiliser ce modèle pour prédire les classes d’images fournies.  \n",
        "3. Appliquer **LIME** et **SHAP** pour expliquer ces prédictions.  \n",
        "4. Comparer les visualisations générées par ces deux outils et analyser les résultats.\n",
        "\n",
        "---\n",
        "\n",
        "## **Plan du TP**\n",
        "\n",
        "### **Étape 1 : Préparation de l’environnement**\n",
        "1. Installer les bibliothèques nécessaires pour PyTorch, LIME, et SHAP.  \n",
        "2. Télécharger ou préparer un ensemble d’images pour les tests (votre dataset préféré, des images issues d’ImageNet, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 2 : Charger un modèle pré-entraîné**\n",
        "1. Nous allons utiliser un modèle pré-entraîné disponible dans PyTorch, comme ResNet18 ou VGG16, avec des poids pré-entraînés sur ImageNet.  \n",
        "2. Le modèle sera utilisé pour effectuer des prédictions sur les images, après les avoir redimensionnées et normalisées en fonction des besoins du modèle.  \n",
        "3. Pour chaque image, nous afficherons les classes prédites avec leurs scores de confiance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 3 : Explicabilité avec LIME**\n",
        "1. **Présentation de LIME :**  \n",
        "   LIME fonctionne en créant des perturbations localisées sur une image (par exemple, en masquant des zones spécifiques) pour mesurer l’impact de chaque zone sur la prédiction.  \n",
        "2. **Étapes :**  \n",
        "   - Segmenter l’image en pixels ou en superpixels.  \n",
        "   - Identifier les zones les plus influentes pour une prédiction donnée.  \n",
        "   - Générer une heatmap mettant en évidence les zones importantes pour la classe prédite.  \n",
        "3. Nous analyserons les résultats pour comprendre quelles parties de l’image influencent le plus la prédiction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 4 : Explicabilité avec SHAP**\n",
        "1. **Présentation de SHAP :**  \n",
        "   SHAP utilise la théorie des jeux pour attribuer une importance à chaque pixel ou groupe de pixels, en mesurant leur contribution à la prédiction.  \n",
        "2. **Étapes :**  \n",
        "   - Fournir les images prétraitées au modèle pour calculer les valeurs SHAP.  \n",
        "   - Générer une visualisation qui montre les contributions positives et négatives des pixels ou des superpixels pour la prédiction.  \n",
        "3. Nous interpréterons les visualisations en observant les zones qui favorisent ou défavorisent la classe prédite.\n",
        "\n",
        "---\n",
        "\n",
        "### **Étape 5 : Comparaison et analyse des résultats**\n",
        "1. **Comparaison des visualisations :**  \n",
        "   - Identifier les différences et similitudes entre les résultats de LIME et SHAP.  \n",
        "   - Analyser les zones mises en évidence par chaque méthode.  \n",
        "2. **Discussion :**  \n",
        "   - Quels sont les points forts et limites de chaque méthode ?  \n",
        "   - Les deux approches donnent-elles des explications cohérentes ?  \n",
        "   - Quels défis rencontrons-nous en utilisant ces outils avec des modèles de grande taille ou des images complexes ?\n",
        "\n",
        "---\n",
        "\n",
        "## **Livrables attendus**\n",
        "1. Les heatmaps générées par LIME et SHAP pour au moins deux images.  \n",
        "2. Une analyse comparative des résultats obtenus avec LIME et SHAP.  \n",
        "---"
      ],
      "metadata": {
        "id": "c3lrZMMsGRNQ"
      }
    }
  ]
}