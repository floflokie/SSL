{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/floflokie/SSL/blob/main/Learning_Representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART I: Learning Representations from Text**\n",
        "\n",
        "In this part of the practical work, students will explore the application of **Word2Vec** for learning word embeddings from a large textual dataset, specifically the **Reuters** corpus. The exercise begins by loading and preprocessing the corpus using **NLTK**, where text data is tokenized and cleaned to prepare it for the model. Once preprocessed, students will train a **Word2Vec** model using **Gensim**, which will learn dense vector representations of words. These embeddings capture semantic relationships between words based on their context in the text. After training, students will explore the learned embeddings by finding similar words to a given input and solving word analogies (e.g., \"king\" is to \"queen\" as \"man\" is to \"woman\"). Finally, students will visualize the learned word embeddings using **Principal Component Analysis (PCA)** to reduce the dimensionality of the embeddings and display them in a 2D space. This visualization will help students understand how the model organizes semantically related words into clusters.\n"
      ],
      "metadata": {
        "id": "ys4M6zsi6G4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Reuters Dataset\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Download and Load the Reuters Dataset:**\n",
        "   - Use the `nltk` library to download and load the **Reuters** corpus. This dataset contains financial news articles which will be used for text processing.\n",
        "   - Ensure that the required resources (such as tokenizers) are downloaded using `nltk.download()`.\n",
        "\n",
        "2. **Dataset Splitting:**\n",
        "   - Separate the dataset into two subsets: one for training and one for testing. This will allow models to be trained on one set and evaluated on another.\n",
        "   - The `reuters.fileids()` method provides a list of document identifiers. Use these to split the documents into training and test sets based on the file identifier (documents that start with \"train\" or \"test\").\n",
        "\n",
        "3. **Preprocess the Text:**\n",
        "   - Write a function that:\n",
        "     - Converts all text to lowercase.\n",
        "     - Tokenizes the text (i.e., splits the text into individual words) using `nltk.tokenize.word_tokenize()`.\n",
        "     - Removes punctuation by filtering out any tokens that are not alphabetic (using `isalpha()`).\n",
        "   - Apply this preprocessing function to both the training and test sets.\n",
        "\n",
        "4. **Display Example Tokens:**\n",
        "   - After preprocessing, display the first 10 tokens of a preprocessed training document to verify that the function is working correctly.\n"
      ],
      "metadata": {
        "id": "49ULT9iy3m2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download the Reuters dataset and required NLTK resources\n",
        "print(\"Downloading the Reuters dataset\")\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load Reuters documents\n",
        "documents = reuters.fileids()\n",
        "\n",
        "# Separate the dataset into training and test sets\n",
        "print(\"Separating the dataset into training and test sets\")\n",
        "train_docs = [reuters.raw(doc_id) for doc_id in documents if doc_id.startswith('train')]\n",
        "test_docs = [reuters.raw(doc_id) for doc_id in documents if doc_id.startswith('test')]\n",
        "\n",
        "# Function to preprocess text: tokenize and clean\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase and tokenize\n",
        "    lowercase = text.lower()\n",
        "    tokens = word_tokenize(lowercase)\n",
        "    # remove punctuation by filtering out any tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # Remove punctuation\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the documents in train and test\n",
        "print(\"Preprocessing\")\n",
        "train_tokens = [preprocess(doc) for doc in train_docs]\n",
        "test_tokens = [preprocess(doc) for doc in test_docs]\n",
        "\n",
        "\n",
        "# Display a few tokens as an example\n",
        "print(\"Example the first 10 tokens of a preprocessed training document:\")\n",
        "print(train_tokens[0][:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPpVTAh9TlRe",
        "outputId": "4024fe0c-99f3-4588-cc0f-9249d93b7827"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the Reuters dataset\n",
            "Separating the dataset into training and test sets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing\n",
            "Example the first 10 tokens of a preprocessed training document:\n",
            "['bahia', 'cocoa', 'review', 'showers', 'continued', 'throughout', 'the', 'week', 'in', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Word2Vec Model\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Train the Word2Vec Model:**\n",
        "   - Use the **Gensim** library to train a Word2Vec model on the preprocessed and tokenized dataset.\n",
        "   - Set the following parameters for the model:\n",
        "     - `vector_size=100`: The dimensionality of the word vectors.\n",
        "     - `window=5`: The maximum distance between the current and predicted word within a sentence.\n",
        "     - `min_count=5`: Ignores words that appear less than 5 times in the corpus.\n",
        "     - `workers=4`: Use 4 CPU threads to speed up training.\n",
        "\n",
        "2. **Save the Trained Model:**\n",
        "   - Once the model is trained, save it to a file for future use. This allows you to load the trained embeddings later without retraining the model from scratch.\n",
        "\n",
        "3. **Check Vocabulary Size:**\n",
        "   - After training, display the size of the vocabulary that the Word2Vec model has learned. The vocabulary consists of all unique words in the corpus that meet the `min_count` threshold.\n"
      ],
      "metadata": {
        "id": "ecvKW8tz3xbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train the Word2Vec model on the tokenized data\n",
        "\n",
        "\n",
        "# Save the trained model\n",
        "word2vec_model.save(\"word2vec_reuters.model\")\n",
        "\n",
        "# Display model information\n",
        "print(f\"Vocabulary size: {len(word2vec_model.wv.key_to_index)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_hFBRyczL1R",
        "outputId": "6bfc44b4-dd6f-436b-f1fe-dbe05d81e747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 8547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Word Embeddings with Word2Vec\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Load the Trained Word2Vec Model:**\n",
        "   - Load the previously trained Word2Vec model from the saved file (`word2vec_reuters.model`). This allows you to use the learned word embeddings for various tasks without having to retrain the model.\n",
        "\n",
        "2. **Find Words Similar to a Given Word:**\n",
        "   - Use the Word2Vec model to find words that are semantically similar to a specified word. In this case, the word \"oil\" is used.\n",
        "   - The model will return a list of words that are closest to \"oil\" based on their embeddings.\n",
        "   \n",
        "3. **Display Similar Words:**\n",
        "   - Print the list of words that the model considers similar to \"oil\" along with their similarity scores.\n"
      ],
      "metadata": {
        "id": "v8eTYaiX39BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained Word2Vec model\n",
        "word2vec_model = Word2Vec.load(\"word2vec_reuters.model\")\n",
        "\n",
        "# Find words similar to a given word\n",
        "similar_words = word2vec_model.wv.most_similar(\"oil\")\n",
        "print(\"Words similar to 'oil':\", similar_words)\n"
      ],
      "metadata": {
        "id": "XaexBZoNzVOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Word Embeddings using PCA\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Select Words to Visualize:**\n",
        "   - Choose a subset of words from the Word2Vec model that you want to visualize. These words should represent different concepts or categories to observe relationships between them.\n",
        "   - For this example, words such as \"market\", \"bank\", \"money\", \"gasoline\", \"pipeline\", \"oil\", \"london\", \"paris\", and \"france\" are selected.\n",
        "\n",
        "2. **Prepare Word Vectors:**\n",
        "   - Extract the word vectors (embeddings) for each selected word from the trained Word2Vec model. Convert this list of vectors into a NumPy array to prepare it for dimensionality reduction.\n",
        "\n",
        "3. **Dimensionality Reduction with PCA:**\n",
        "   - Perform **Principal Component Analysis (PCA)** to reduce the dimensionality of the word embeddings from the original high-dimensional space (e.g., 100 dimensions) to 2 dimensions.\n",
        "   - This allows for easy visualization of the embeddings in a 2D plane.\n",
        "\n",
        "4. **Visualize the Embeddings in 2D:**\n",
        "   - Plot the 2D projections of the word embeddings using **Matplotlib**.\n",
        "   - Annotate each point on the scatter plot with its corresponding word to show how similar or different words are positioned relative to each other.\n",
        "   \n",
        "5. **Interpret the Plot:**\n",
        "   - Once the plot is generated, observe the spatial relationships between words. Words that are semantically similar or belong to the same category should be closer to each other in the 2D space.\n"
      ],
      "metadata": {
        "id": "9I3mL0FY4KeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select a subset of words to visualize\n",
        "words = [\"market\", \"bank\", \"money\", \"gasoline\", \"pipeline\", \"oil\", \"london\", \"paris\", \"france\"]\n",
        "word_vectors = [word2vec_model.wv[word] for word in words]\n",
        "\n",
        "# Convert the list of word vectors to a NumPy array\n",
        "\n",
        "\n",
        "# Perform PCA to reduce the dimensionality to 2 components\n",
        "\n",
        "\n",
        "# Visualize the embeddings in 2D\n",
        "\n",
        "\n",
        "# Annotate the points with words\n",
        "\n",
        "\n",
        "plt.title(\"2D Visualization of Word Embeddings using PCA\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3b1A7psXzZx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART II: Learning Representations from Images**\n",
        "\n",
        "In this part of the practical work, students will build and train a **Masked Autoencoder (MAE)** using convolutional neural networks (CNNs) to reconstruct images with missing parts. The exercise begins by loading the **CIFAR-10** dataset, normalizing the pixel values, and applying random masks to the images to hide a portion of each one. Students will then define a simple CNN-based autoencoder, where the **encoder** extracts dense features from the image and the **decoder** reconstructs the masked image. Once the model is trained, students will extract and explore the **dense representations** (i.e., the encoded features) learned by the model. These representations are visualized using **t-SNE**, a dimensionality reduction technique, to project the high-dimensional representations into 2D space. The final visualization allows students to see how the model organizes different image classes and whether similar images cluster together based on their learned representations.\n"
      ],
      "metadata": {
        "id": "CA-rSsQ36egj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking a Portion of the Image for Input Preprocessing\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Use GPU if Available:**\n",
        "   - The code checks if a **GPU** is available for faster computations. If a GPU is available, the device is set to **cuda**; otherwise, the **CPU** is used. This is important for accelerating deep learning tasks.\n",
        "\n",
        "2. **Masking Part of the Image:**\n",
        "   - The `mask_image` function is used to hide a portion of each input image, which will be later used for learning. This masking is a preprocessing step to manipulate the input data.\n",
        "\n",
        "3. **Masking Function:**\n",
        "   - The `mask_image` function:\n",
        "     - Takes an image as input and applies a random **mask** to hide part of the image.\n",
        "     - The `mask_ratio` controls how much of the image is masked. A mask ratio of `0.80` means that 80% of the image will be hidden.\n",
        "     - The function generates a random binary mask, where values greater than `mask_ratio` are kept (i.e., visible), and the others are masked.\n",
        "   \n",
        "4. **Application of the Mask:**\n",
        "   - The masked image is then prepared for further processing by multiplying the binary mask with the original image.\n",
        "   - The masked image is moved to the appropriate device (GPU or CPU) for efficient computation.\n",
        "   \n",
        "### Purpose:\n",
        "This step prepares the data by masking parts of the input image, which will be later used for training a model to reconstruct or predict the missing regions.\n"
      ],
      "metadata": {
        "id": "wl8f5I_H4oa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Function to mask part of the image (for self-supervised learning)\n",
        "def mask_image(image, mask_ratio=0.80):\n",
        "    # Generate a random mask and apply it to the image\n",
        "\n",
        "    return image * mask"
      ],
      "metadata": {
        "id": "vU2CCTcV2bxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a Simple CNN-Based Masked Autoencoder (MAE)\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Define the Autoencoder Architecture:**\n",
        "   - This part of the code defines a **Masked Autoencoder (MAE)** using a simple convolutional neural network (CNN).\n",
        "   - The autoencoder is composed of two main components:\n",
        "     - **Encoder**: A CNN-based architecture that extracts features from the input image.\n",
        "     - **Decoder**: A transposed convolutional network that reconstructs the image from the extracted features, specifically focusing on reconstructing the masked parts of the image.\n",
        "\n",
        "2. **Encoder:**\n",
        "   - The encoder consists of two convolutional layers followed by **ReLU** activations and **MaxPooling** layers:\n",
        "     - The first convolution layer extracts 16 feature maps from the input image.\n",
        "     - The second convolution layer extracts 32 feature maps and reduces the spatial size of the image using max pooling.\n",
        "   - The encoder learns to extract **dense representations** (features) of the input image.\n",
        "\n",
        "3. **Decoder:**\n",
        "   - The decoder is responsible for **reconstructing** the masked parts of the input image.\n",
        "   - It consists of two transposed convolutional layers followed by **ReLU** activations:\n",
        "     - The first layer upsamples the encoded features, and the second layer reconstructs the output with 3 channels (RGB image).\n",
        "     - The **Sigmoid** activation normalizes the pixel values between 0 and 1, matching the input image format.\n",
        "\n",
        "4. **Forward Pass:**\n",
        "   - In the `forward` method:\n",
        "     - The image is passed through the encoder to extract dense representations.\n",
        "     - The decoder uses these representations to reconstruct the masked input.\n",
        "   - The method returns both the encoded features and the reconstructed image.\n",
        "\n",
        "### Purpose:\n",
        "This autoencoder architecture will be used to learn how to reconstruct masked images, forcing the encoder to learn meaningful representations of the data even when parts of the input are missing. The reconstruction task helps in training a model that can generalize to understanding various image structures and patterns.\n"
      ],
      "metadata": {
        "id": "xm7R9L7V44Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple CNN-based MAE (Masked Autoencoder)\n",
        "class SimpleMAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMAE, self).__init__()\n",
        "        # Encoder (simple CNN to extract image features)\n",
        "        self.encoder = nn.Sequential(\n",
        "\n",
        "\n",
        "        )\n",
        "        # Decoder (reconstruct the masked part of the image)\n",
        "        self.decoder = nn.Sequential(\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)  # Extract dense representations\n",
        "        decoded = self.decoder(encoded)  # Reconstruct the image\n",
        "        return encoded, decoded  # Return both the encoding and the reconstruction"
      ],
      "metadata": {
        "id": "HwRp8DYX2hhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading CIFAR-10 Dataset and Training the Masked Autoencoder\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Load and Preprocess the CIFAR-10 Dataset:**\n",
        "   - The **CIFAR-10** dataset is a collection of 60,000 32x32 color images, divided into 10 classes. It is often used for image classification and reconstruction tasks.\n",
        "   - The images are loaded using the **Torchvision** library. Each image is:\n",
        "     - Converted to a **tensor** using `transforms.ToTensor()`.\n",
        "     - **Normalized** to have pixel values between -1 and 1 using `transforms.Normalize()` with a mean and standard deviation of 0.5 for each color channel (RGB).\n",
        "\n",
        "2. **Create DataLoader:**\n",
        "   - A **DataLoader** is created to load batches of 64 images from the training set. The data is shuffled to ensure randomness in the training process.\n",
        "\n",
        "3. **Model Instantiation:**\n",
        "   - The **SimpleMAE** model (Masked Autoencoder) is instantiated and moved to the GPU (if available) for faster training.\n",
        "\n",
        "4. **Define Loss Function and Optimizer:**\n",
        "   - The loss function used is **Mean Squared Error (MSE)**, which measures the difference between the reconstructed image and the original image.\n",
        "   - The optimizer is **Adam**, with a learning rate of 0.001, which adjusts the model weights during training to minimize the loss.\n",
        "\n",
        "5. **Training the Model:**\n",
        "   - The training process is defined in the `train_model()` function. This function trains the model for a specified number of epochs (in this case, 5 epochs):\n",
        "     1. **Masking**: Each image in the batch is masked (with 75% of the pixels hidden) using the `mask_image` function.\n",
        "     2. **Forward Pass**: The masked images are passed through the model, which attempts to reconstruct the original image.\n",
        "     3. **Loss Calculation**: The difference between the reconstructed image and the original image is computed using the MSE loss.\n",
        "     4. **Backpropagation**: The gradients are computed, and the optimizer updates the model parameters to minimize the loss.\n",
        "     5. **Logging**: After each epoch, the average loss for that epoch is printed to track the model's progress.\n",
        "\n",
        "6. **Training Loop:**\n",
        "   - The model is trained over 5 epochs, and after each epoch, the loss value is printed to monitor the model's learning process.\n",
        "\n",
        "### Purpose:\n",
        "The purpose of this training process is to allow the model to learn to reconstruct images with parts of them masked. By minimizing the reconstruction loss, the model is forced to learn useful image representations, even when large parts of the input are missing.\n"
      ],
      "metadata": {
        "id": "LjUUzLOM5Ewe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset and preprocess it (normalize pixel values)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate the model and move it to GPU\n",
        "model = SimpleMAE().to(device)\n",
        "\n",
        "# Define loss function (Mean Squared Error for image reconstruction) and optimizer (Adam)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, _ in train_loader:\n",
        "            # Move images to GPU\n",
        "\n",
        "\n",
        "            # Mask part of the images for training\n",
        "\n",
        "\n",
        "            # Move masked images to GPU\n",
        "\n",
        "\n",
        "            # Train the model\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Focus on reconstruction\n",
        "\n",
        "            # Compare to original image\n",
        "\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "# Train the model for 5 epochs\n",
        "train_model(model, epochs=5)\n"
      ],
      "metadata": {
        "id": "Iic8RI_12i1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Dense Representations from the Model\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Model in Evaluation Mode:**\n",
        "   - Before extracting dense representations, the model is switched to **evaluation mode** using `model.eval()`. This disables certain layers like dropout and batch normalization that are only used during training, ensuring the model behaves consistently for evaluation.\n",
        "\n",
        "2. **Disable Gradient Calculation:**\n",
        "   - The `torch.no_grad()` context is used to disable gradient computation. This reduces memory usage and speeds up the process since no backpropagation is needed when simply extracting features.\n",
        "\n",
        "3. **Mask Images and Extract Representations:**\n",
        "   - For each batch of images from the DataLoader:\n",
        "     1. The images are moved to the **GPU** for efficient processing.\n",
        "     2. Each image is **masked** using the `mask_image` function (e.g., 75% of the pixels are hidden).\n",
        "     3. The masked images are passed through the **encoder** part of the model to extract their **dense features** (i.e., encoded representations).\n",
        "     4. The encoded features are **flattened** and moved to the CPU for further processing.\n",
        "   \n",
        "4. **Store Representations and Labels:**\n",
        "   - The **encoded representations** are appended to a list, and the corresponding **labels** (i.e., the class of the image) are also stored.\n",
        "   - This allows us to associate each dense representation with the original label for later tasks like visualization or classification.\n",
        "\n",
        "5. **Return the Results:**\n",
        "   - After processing all batches, the dense representations are concatenated into a single array and returned along with their corresponding labels.\n",
        "\n",
        "### Purpose:\n",
        "The purpose of this function is to extract the **dense representations** learned by the encoder from the **Masked Autoencoder (MAE)**. These representations capture the most important features of the images, and they can be used for various tasks such as clustering, classification, or visualization of learned features.\n"
      ],
      "metadata": {
        "id": "6HU8Q62T5QSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract dense representations after training\n",
        "def extract_dense_representations(model, loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    dense_representations = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, image_labels in loader:\n",
        "            images = images.to(device)\n",
        "            masked_images = torch.stack([mask_image(img, mask_ratio=0.75) for img in images]).to(device)\n",
        "\n",
        "            # Extract dense features (encoded representations)\n",
        "            encoded, _ = model(masked_images)\n",
        "            dense_representations.append(encoded.view(encoded.size(0), -1).cpu())  # Flatten and move to CPU\n",
        "            labels.extend(image_labels.numpy())\n",
        "\n",
        "    # Return dense representations and corresponding labels\n",
        "    dense_representations = torch.cat(dense_representations).numpy()\n",
        "    return dense_representations, labels\n"
      ],
      "metadata": {
        "id": "71Zp-ecj2r4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Dense Representations Using t-SNE\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Randomly Sample Images:**\n",
        "   - A subset of 1000 images is randomly sampled from the **CIFAR-10** dataset for visualization.\n",
        "   - The `np.random.choice()` function is used to select random indices from the dataset, and a `SubsetRandomSampler` ensures that only these images are loaded for processing.\n",
        "\n",
        "2. **Extract Dense Representations:**\n",
        "   - The **dense representations** for the sampled images are extracted using the `extract_dense_representations` function. This function passes the images through the trained autoencoder and retrieves the encoded features from the encoder.\n",
        "   - The corresponding **labels** for each image are also retrieved for color-coding during visualization.\n",
        "\n",
        "3. **Dimensionality Reduction with t-SNE:**\n",
        "   - Since the dense representations are high-dimensional, **t-SNE** (t-distributed Stochastic Neighbor Embedding) is used to reduce the dimensionality to 2 components, making it easier to visualize.\n",
        "   - t-SNE projects the data points into a 2D space while preserving local similarities between points, making it ideal for visualizing clusters of similar representations.\n",
        "\n",
        "4. **Visualize the 2D Representation:**\n",
        "   - The 2D projection of the dense representations is visualized using **Matplotlib**.\n",
        "   - A scatter plot is created where each point represents a different image, and the points are color-coded according to their class labels (e.g., \"oil\", \"market\", etc.).\n",
        "   - A color bar is added to the plot to indicate the class labels associated with each color.\n",
        "   \n",
        "5. **Interpret the Plot:**\n",
        "   - After visualization, observe how the representations are organized in 2D space:\n",
        "     - Images from the same class should ideally cluster together, showing that the model has learned useful representations for distinguishing between different image categories.\n",
        "\n",
        "### Purpose:\n",
        "The purpose of this function is to visualize the **dense representations** learned by the model using **t-SNE**. This allows you to understand how well the model has learned to represent different classes of images and how distinct these representations are.\n"
      ],
      "metadata": {
        "id": "sbHSb_mG5aIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize dense representations with t-SNE\n",
        "def visualize_dense_representations(model, loader, num_samples=1000):\n",
        "    # Randomly sample 1000 images\n",
        "    indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n",
        "    sampler = SubsetRandomSampler(indices)\n",
        "    sample_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
        "\n",
        "    # Extract dense representations for the sample\n",
        "    dense_representations, labels = extract_dense_representations(model, sample_loader)\n",
        "\n",
        "    # Perform dimensionality reduction with t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    reduced_representations = tsne.fit_transform(dense_representations)\n",
        "\n",
        "    # Visualize the 2D representation of dense features\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(reduced_representations[:, 0], reduced_representations[:, 1], c=labels, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('t-SNE of learned dense representations (sample of 1000 images)')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the representations (sample of 1000 images)\n",
        "visualize_dense_representations(model, train_loader, num_samples=1000)\n"
      ],
      "metadata": {
        "id": "kpl3X0pt24Bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}